import torch
import torch.nn as nn
import torch.nn.functional as F

def get_torch_trans(heads=8, layers=1, channels=64):
    """
    Creates a Transformer encoder module to process MTS timestamps/features as sequences.
    
    Parameters:
    - heads (int): Number of attention heads.
    - layers (int): Number of encoder layers.
    - channels (int): Dimensionality of the model (d_model in Transformer terminology).
    
    Returns:
    - nn.TransformerEncoder: A Transformer encoder object.
    """
    encoder_layer = nn.TransformerEncoderLayer(
        d_model=channels, nhead=heads, dim_feedforward=64, activation="gelu"
    )
    return nn.TransformerEncoder(encoder_layer, num_layers=layers, enable_nested_tensor=False)


def Conv1d_with_init(in_channels, out_channels, kernel_size):
    """
    Initializes a 1D convolutional layer with Kaiming normal initialization.
    
    Parameters:
    - in_channels (int): Number of channels in the input signal.
    - out_channels (int): Number of channels produced by the convolution.
    - kernel_size (int): Size of the convolving kernel.
    
    Returns:
    - nn.Conv1d: A 1D convolutional layer with weights initialized.
    """
    layer = nn.Conv1d(in_channels, out_channels, kernel_size)
    nn.init.kaiming_normal_(layer.weight)
    return layer

class embedding_MTS(nn.Module):
    """
    An embedding module for multivariate time series data, incorporating both temporal and spatial
    encoding via Transformer encoders and convolutional layers, it corresponds to the Embedding block in TSDE architecture.
    
    Parameters:
    - config (dict): A configuration dictionary containing model parameters such as number of channels,
                     embedding dimensions, and number of heads for the Transformer encoders in the embedding block.
    """


    def __init__(self, config):
        super().__init__()
        self.channels = config["channels"]
        self.timeemb = config["timeemb"]
        self.featureemb = config["featureemb"]
        self.emb_size = self.channels + self.timeemb + self.featureemb
        self.time_layer = get_torch_trans(heads=config["nheads"], layers=1, channels=self.emb_size) ## Temporal encoder
        self.feature_layer = get_torch_trans(heads=config["nheads"], layers=1, channels=self.emb_size) ## Spatial encoder
        
        self.input_projection = Conv1d_with_init(1, self.channels, 1)
        
        self.xt_projection = Conv1d_with_init(self.emb_size, self.channels, 1)
        self.xf_projection = Conv1d_with_init(self.emb_size, self.channels, 1)
        

    def forward_time(self, x, base_shape):
        """
        Processes the input data through the temporal Transformer encoder (timestamps are considered as tokens).
        
        Parameters:
        - x (Tensor): The observed part of the MTS combined with timestamps embedding and features embedding as tensor.
        - base_shape (tuple): The base shape of the input tensor before reshaping.
        
        Returns:
        - Tensor: The temporally encoded tensor.
        """
        B, C, K, L = base_shape
        if L == 1:
            return x
        x = x.permute(0, 2, 1, 3).reshape(B * K, C, L)
        x = self.time_layer(x.permute(2, 0, 1)).permute(1, 2, 0)
        x = x.reshape(B, K, C, L).permute(0, 2, 1, 3)
        return x

    def forward_feature(self, x, base_shape):
        """
        Processes the input data through the spatial Transformer encoder (features are considered as tokens).
        
        Parameters:
        - x (Tensor): The observed part of the MTS combined with timestamps embedding and features embedding as tensor.
        - base_shape (tuple): The base shape of the input tensor before reshaping.
        
        Returns:
        - Tensor: The spatially encoded tensor.
        """
        B, C, K, L = base_shape
        if K == 1:
            return x
        x = x.permute(0, 3, 1, 2).reshape(B * L, C, K)
        x = self.feature_layer(x.permute(2, 0, 1)).permute(1, 2, 0)
        x = x.reshape(B, L, C, K).permute(0, 2, 3, 1)
        return x

    def forward(self, x, time_embed, feature_embed):
        """
        The forward pass of the embedding module, processing multivariate time series data with
        temporal and spatial embeddings.
        
        Parameters:
        - x (Tensor): The observed part of the MTS.
        - time_embed (Tensor): The time embeddings tensor.
        - feature_embed (Tensor): The feature embeddings tensor.
        
        Returns:
        - Tuple[Tensor, Tensor, Tensor]: A tuple containing the combined temporal and spatial embeddings generated by the two transformer encoders, 
                                         the processed temporal embedding, and the processed spatial embedding.
        """
        B, _, K, L = x.shape
        time_embed = time_embed.unsqueeze(2).expand(-1, -1, K, -1)
        feature_embed = feature_embed.unsqueeze(1).expand(-1, L, -1, -1)
        
        x = x.reshape(B, 1, K * L)
        x = self.input_projection(x) ## First Convolution before fedding the data to the encoders
        x = F.relu(x)                
        x = x.reshape(B, self.channels, K, L)
        x = torch.cat([x, time_embed.permute(0, 3, 2, 1), feature_embed.permute(0, 3, 2, 1)], dim=1)
        
        base_shape = B, x.shape[1], K, L
        
        xt = self.forward_time(x, base_shape)
        xf = self.forward_feature(x, base_shape)
        
        xt = self.forward_time(xf, base_shape)
        xf = self.forward_feature(xt, base_shape)
        
        
        xt_reshaped = xt.reshape(B, base_shape[1], K*L)
        xt_proj = F.silu(self.xt_projection(xt_reshaped))
        xt = xt_proj.reshape(B, self.channels, K, L)
        
         
        xf_reshaped = xf.reshape(B, base_shape[1], K*L)
        xf_proj = F.silu(self.xf_projection(xf_reshaped))
        xf = xf_proj.reshape(B, self.channels, K, L)
        

        x = torch.cat([xt, xf], dim=1) # B, 2*C, K, L 
        
        return x, xt_proj, xf_proj